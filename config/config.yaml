models:
  bert:
    name: "bert-base-uncased"
    model_type: "bert"
    max_length: 512
  roberta:
    name: "roberta-base"
    model_type: "roberta"
    max_length: 512
  distilbert:
    name: "distilbert-base-uncased"
    model_type: "distilbert"
    max_length: 512

compression:
  masking_probabilities: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
  quantization_bits: 8
  batch_size: 16

training:
  epochs: 50
  learning_rate: 5e-5
  warmup_steps: 500
  weight_decay: 0.01
  gradient_accumulation_steps: 2
  fp16: true

data:
  train_size: 10000
  test_size: 1000
  max_length: 256
  dataset_name: "wikitext"
  dataset_config: "wikitext-2-raw-v1"

evaluation:
  metrics:
    - "reconstruction_accuracy"
    - "perplexity"
    - "rouge"
    - "bert_score"
    - "semantic_similarity"
  save_reconstructions: true

experiment:
  seed: 42
  num_runs: 3
  device: "cuda"
  output_dir: "./results"
  save_models: true
  logging_steps: 100

visualization:
  figure_format: "png"
  dpi: 300
  style: "seaborn-v0_8-darkgrid"
  save_figures: true

experiment_1b:
  corpus_size: 5000
  test_size: 500
  factual_probes: 1000
  oracle_model: "gpt2-large"

  tolerance_thresholds:
    high_fidelity_factual: 0.95
    creative_reasoning: 0.85
    acceptable_degradation: 0.10

  fine_tune_sizes: [10, 50, 100, 500, 1000, 5000]
  target_performance: 0.80
  
# Experiment 1C specific settings
experiment_1c:
  # Pruning configurations
  pruning:
    levels: [0.0, 0.1, 0.3, 0.5, 0.7, 0.9]
    type: "magnitude"  # or "structured"
    
  # GLUE evaluation settings
  glue:
    tasks: ["sst2", "mrpc", "rte"]
    train_samples: 500
    eval_samples: 200
    fine_tune_epochs: 100
    
  # Performance benchmarking
  benchmark:
    batch_size: 8
    num_inference_runs: 50
    
  # Structural fidelity settings
  structural_fidelity:
    test_samples: 20
    masking_probability: 0.3
    
  # Sweet spot analysis weights
  sweet_spot_weights:
    downstream_performance: 0.4
    structural_fidelity: 0.3
    computational_efficiency: 0.3
